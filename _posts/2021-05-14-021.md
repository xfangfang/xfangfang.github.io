---
layout: post
title: M1版本Macbook机器学习究竟有多强
tags: [keras, mac, ml]
categories: mac
---

开篇提醒：本文系无机云评测。

### 前情提要

今天在知乎上看到了 [Apple Silicon M1 机器学习性能简单测试](https://zhuanlan.zhihu.com/p/350955566)，突然沸腾了（沸腾体警告⚠️）。文章中用一段cnn训练mnist的代码在几台机器上分别运行，以每batch运行时间为评测标准，横向对比。正好我也把手头的几个电脑加入这个对比中。

### 运行结果

把文章中的结果，和我跑出来的结果放在一起，我最感兴趣的数据就是mbp的GPU对比m1，即使他是一个不十分准确的对比也让我对m1有了个粗略的认识。

|                                                | CPU    | GPU  | TPU  |
| ---------------------------------------------- | ------ | ---- | ---- |
| Macbook Air 2020 M1                            |        | 20s  |      |
| Macbook Pro 13 2015（i5 5257u ?）              | 310s ? |      |      |
| MacBook Air 13 2015（i5 5250u）                | 85s    |      |      |
| Macbook Pro 15 2018（i7 8750h; RadeonPro555x） | 44s    | 26s  |      |
| Colab GPU \|\| TPU                             |        | 8s   | 134s |
| xfangfang's PC（Ryzen5 3600; 2070super）       | 22s    | 2s   |      |

<!--注：上面的数据比较让人迷惑的是原文中mbp2015的数据，我正好有一台同年的mba，即使他日常使用已经有一些卡顿了，跑一个batch的时间也没有原文中的mbp长，我猜或许是原文mbp使用的tensorflow版本不对吧，或者是什么奇怪的原因。（肯定不是脚本的问题，我试着用我改过的脚本和原始的脚本对照运行，二者几乎没有差距）-->

说的有些迟到，但是好像ARM的时代真的来了。手上这台18款mbp的CPU发热巨大，日常使用60度，动不动就跑到100度，CPU跑这个卷积更是风扇满转，不过切到GPU去运行倒是完全不热。日常的高温让我爱不起来，看了数据甚至有点想入手下一代ARM处理器的Macbook。



### 附：评测脚本

为了可以跑在我的电脑上，稍微修改了一些，应该和知乎文章中的脚本差不多。


```python
# 运行在macbook上 使用plaidml keras开启AMD GPU支持
# pip install plaidml-keras
# terminal 运行 plaidml-setup 选择对应的显卡
import plaidml.keras
plaidml.keras.install_backend()
import os
os.environ["KERAS_BACKEND"] = "plaidml.keras.backend"
import keras
```


```python
# 通用脚本 CPU
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
import tensorflow
keras = tensorflow.keras
```


```python
# 通用脚本 GPU
gpus = tf.config.experimental.list_physical_devices('GPU')
assert len(gpus) > 0, "Not enough GPU hardware devices available"
tf.config.experimental.set_memory_growth(gpus[0], True)
```


上面三段代码片段，分别和下面的脚本组合，用来在不同设备上运行



```python
import time
import numpy as np
from datetime import timedelta

mnist = keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = np.expand_dims(train_images, axis=3) / 255.0
test_images = np.expand_dims(test_images, axis=3) / 255.0

model = keras.models.Sequential([
    keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu',input_shape=(28,28,1)),
    keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
    keras.layers.MaxPooling2D(pool_size=(2, 2)),
    keras.layers.Flatten(),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=keras.optimizers.Adam(0.001),
    metrics=['accuracy'],
)
start = time.time()
model.fit(
    train_images,
    train_labels,
    epochs=10,
    batch_size=128
)
delta = (time.time() - start)
elapsed = str(timedelta(seconds=delta))
print('Elapsed Time: {}'.format(elapsed))
```

